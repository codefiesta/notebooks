{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc34d188-f177-4f14-905c-b516ef8f08fe",
   "metadata": {},
   "source": [
    "# Named Entity Patterns\n",
    "\n",
    "Named Entity patterns are dictionaries with two keys: \"label\", specifying the label to assign to the entity if the pattern is matched, and \"pattern\", the match pattern. The entity ruler accepts two types of patterns:\n",
    "\n",
    "```json\n",
    "{\"label\": \"CON-BIM-CATG\", \"pattern\": [{\"LOWER\": \"air\"}, {\"LOWER\": \"terminals\"}]}\n",
    "{\"label\": \"CON-BIM-CATG\", \"pattern\": [{\"LOWER\": \"air\"}, {\"LEMMA\": \"terminal\"}]}\n",
    "{\"label\": \"CON-BIM-CATG\", \"pattern\": [{\"LOWER\": \"electrical\"}, {\"LOWER\": \"equipment\"}]}\n",
    "{\"label\": \"CON-BIM-CATG\", \"pattern\": [{\"LOWER\": \"electrical\"}, {\"LEMMA\": \"equipment\"}]}\n",
    "````\n",
    "\n",
    "## Objectives\n",
    " - Generate a pattern file (in `.jsonl` format) that can be applied to a spaCy EntityRuler component that can easily combine rule-based and statistical named entity recognition for even more powerful pipelines.\n",
    " - Generate sample training data from the phrase templates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfb8187-b96c-4f15-b1c7-e6d1b9361972",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "747ce3db-a7ff-4918-a667-75d789f1955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import inflect\n",
    "import random\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.training import (\n",
    "    offsets_to_biluo_tags,\n",
    "    biluo_to_iob\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from utilities import (\n",
    "    csv_to_list,\n",
    "    jsonl_to_list,\n",
    "    write_jsonl\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4898763-e233-4698-888e-811a6493bf35",
   "metadata": {},
   "source": [
    "### Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dabf2012-797a-4680-b944-23ff06bb40bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the natural language pipeline\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Init the inflection engine\n",
    "infection = inflect.engine()\n",
    "\n",
    "def is_plural(word):\n",
    "    tokens = nlp(word)\n",
    "    last_token = tokens[-1]\n",
    "    tag = last_token.tag_\n",
    "    return tag == 'NNPS' or tag == 'NNS' or tag == 'NN'\n",
    "    # \"\"\"\n",
    "    # Tests if a word is plural.\n",
    "    \n",
    "    # Args:\n",
    "    #     word (str): The word to test\n",
    "    # \"\"\"\n",
    "    # singular = infection.singular_noun(word)\n",
    "    # # Returns true if plural, false if singular or unrecognized\n",
    "    # return bool(singular)\n",
    "\n",
    "def make_plural(word):\n",
    "    \"\"\"\n",
    "    Pluralizes a string.\n",
    "\n",
    "    Args:\n",
    "        word (str): The string to pluralize.\n",
    "    \"\"\"\n",
    "    return infection.plural_noun(word)\n",
    "\n",
    "def make_patterns(label, text):\n",
    "    \"\"\"\n",
    "    Makes an array of label pattern for the given text.\n",
    "\n",
    "    Args:\n",
    "        label (str): the label to apply to the text pattern\n",
    "        text (str): The text pattern\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = [token for i, token in enumerate(doc)]\n",
    "    tokens_len = len(tokens)-1\n",
    "    patterns = []\n",
    "\n",
    "    # Make the lower case patters\n",
    "    lower_case_patterns = list(map(lambda x: {\"LOWER\": x.text.lower()}, tokens))\n",
    "    \n",
    "    # Lemmatize the text\n",
    "    lemma_patterns = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if i == tokens_len:\n",
    "            if token.text.lower() != token.lemma_.lower():\n",
    "                lemma_patterns.append({'LEMMA': token.lemma_.lower()})\n",
    "            else:\n",
    "                lemma_patterns.append({'LEMMA': token.text.lower()})\n",
    "        else:\n",
    "            lemma_patterns.append({'LOWER': token.text.lower()})\n",
    "\n",
    "    patterns.append({\"label\": label, \"pattern\": lower_case_patterns})\n",
    "    patterns.append({\"label\": label, \"pattern\": lemma_patterns})\n",
    "    return patterns\n",
    "\n",
    "def make_bim_label_patterns(named_entities):\n",
    "    \"\"\"\n",
    "    Makes a list of BIM label patterns to apply for use in training an ML model.\n",
    "\n",
    "    Args:\n",
    "        con_ner_tags (list): A list of con-NER tags\n",
    "    \"\"\"\n",
    "    csv_file = './data/revit_categories_families_types.csv'\n",
    "    cat_fam_types = csv_to_list(csv_file)\n",
    "    data = []\n",
    "\n",
    "    # Create a unique set of all the categories\n",
    "    categories = sorted(list(set().union(str(item['category']) for item in cat_fam_types)))\n",
    "\n",
    "    # Map all of the categories\n",
    "    label = named_entities[0]['label']\n",
    "    patterns = list(map(lambda x: make_patterns(label, x), categories))\n",
    "    flattened_list = [item for sublist in patterns for item in sublist]\n",
    "    for item in flattened_list:\n",
    "        data.append(item)\n",
    "    \n",
    "    for category in categories:\n",
    "        \n",
    "        # Create a unique set of the families in this category\n",
    "        families = set().union([str(d['family']) for d in cat_fam_types if d['category'] == category])\n",
    "\n",
    "        # Map all of the families\n",
    "        label = named_entities[1]['label']\n",
    "        patterns = list(map(lambda x: make_patterns(label, x), families))\n",
    "        flattened_list = [item for sublist in patterns for item in sublist]\n",
    "        for item in flattened_list:\n",
    "            data.append(item)\n",
    "\n",
    "        for family in families:\n",
    "\n",
    "            # Create a unique set of types in this family\n",
    "            types = set().union([str(d['type']) for d in cat_fam_types if d['family'] == family])\n",
    "            \n",
    "            # Map all of the types\n",
    "            label = named_entities[2]['label']\n",
    "            patterns = list(map(lambda x: make_patterns(label, x), types))\n",
    "            flattened_list = [item for sublist in patterns for item in sublist]\n",
    "            for item in flattened_list:\n",
    "                data.append(item)\n",
    "\n",
    "\n",
    "    # Make Levels labels\n",
    "    label = named_entities[4]['label']\n",
    "    patterns = make_patterns(label, 'Level')\n",
    "    data += patterns\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_rule_patterns_file():\n",
    "    \"\"\"\n",
    "    Makes a list of label patterns to apply for use in training an ML model.\n",
    "    \"\"\"\n",
    "\n",
    "    jsonl_file = './data/named_entities.jsonl'\n",
    "    named_entities = jsonl_to_list(jsonl_file)\n",
    "    data = []\n",
    "\n",
    "    # Make the BIM labels\n",
    "    data += make_bim_label_patterns(named_entities)\n",
    "\n",
    "    return data\n",
    "\n",
    "def make_training_data():\n",
    "    \"\"\"\n",
    "    Makes a list of training data.\n",
    "    \"\"\"\n",
    "    jsonl_file = './data/phrase_examples.jsonl'\n",
    "    examples = jsonl_to_list(jsonl_file)\n",
    "\n",
    "    csv_file = './data/revit_categories_families_types.csv'\n",
    "    cat_fam_types = csv_to_list(csv_file)\n",
    "    # Create a unique set of all the categories\n",
    "    categories = sorted(list(set().union(str(item['category']) for item in cat_fam_types)))\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    for i, category in enumerate(categories):\n",
    "        # Create a unique set of the families in this category\n",
    "        families = set().union([str(d['family']) for d in cat_fam_types if d['category'] == category])\n",
    "\n",
    "        for example in examples:\n",
    "            text = example['text']    \n",
    "            row = copy.deepcopy(example)\n",
    "            row['text'] = text.format(category.lower())\n",
    "            data.append(row)\n",
    "\n",
    "        for family in families:\n",
    "\n",
    "            result = family\n",
    "\n",
    "            # Modify the text based on the family\n",
    "            tokens = nlp(family)\n",
    "            first_token = tokens[0]\n",
    "            last_token = tokens[-1]\n",
    "            first_tag = first_token.tag_ \n",
    "            last_tag = last_token.tag_\n",
    "            is_noun = last_tag.startswith('N')\n",
    "            is_plural = last_tag == 'NNPS' or last_tag == 'NNS'\n",
    "            is_singular_mass = last_tag == 'NN'\n",
    "            is_modified_noun = is_noun and first_tag == 'JJ'\n",
    "        \n",
    "            if is_plural == False:\n",
    "                result = make_plural(family)\n",
    "\n",
    "            # The family isn't a noun so concat the family + category\n",
    "            if is_noun == False:\n",
    "                result = family + ' ' + category\n",
    "        \n",
    "            # The family is a modified noun so concat the family + category\n",
    "            if is_modified_noun == True:\n",
    "                # Check to see if the last pluralized token equals the category\n",
    "                pluralized = make_plural(last_token.text).strip().lower()\n",
    "                if pluralized.lower() != category.lower():\n",
    "                    result = family + ' ' + category\n",
    "\n",
    "            for example in examples:\n",
    "                text = example['text']\n",
    "                row = copy.deepcopy(example)\n",
    "                row['text'] = text.format(result.lower())\n",
    "                data.append(row)\n",
    "\n",
    "\n",
    "    navigation_examples = jsonl_to_list('./data/navigation_examples.jsonl')\n",
    "\n",
    "    # Balance the data set by simply repeating the number of navigation commands\n",
    "    count = round((len(data) / 3) /6)\n",
    "    \n",
    "    # Make Navigation Data\n",
    "    for i in range(count):\n",
    "        for item in navigation_examples:\n",
    "            data.append(item)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08391cd-25a5-4deb-a73b-3c7f3540b635",
   "metadata": {},
   "source": [
    "### Generate Patterns File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2c8492f-8daf-4cfd-8d87-1215791a063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated custom patterns: 640\n"
     ]
    }
   ],
   "source": [
    "PATTERNS_OUTPUT_FILE = './data/patterns.jsonl'\n",
    "\n",
    "# Generate the rule patterns data\n",
    "data = make_rule_patterns_file()\n",
    "# Write the dataset out to a jsonl file\n",
    "write_jsonl(data, PATTERNS_OUTPUT_FILE)\n",
    "\n",
    "# Test applying the patterns to an entity ruler\n",
    "if nlp.has_pipe(\"entity_ruler\") == False:\n",
    "    config = {\"overwrite_ents\": True}\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", config=config).from_disk(PATTERNS_OUTPUT_FILE)\n",
    "    print(\"✅ Generated custom patterns:\", len(ruler.patterns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffdf216-6ca2-418e-823c-cb9a797f482a",
   "metadata": {},
   "source": [
    "### Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "360e9f7c-d50e-495f-aa49-65e2b1cd7a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated training data: 20121\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DATA_OUTPUT_FILE = './data/training.jsonl'\n",
    "data = make_training_data()\n",
    "#random.shuffle(data)\n",
    "# Write the dataset out to a jsonl file\n",
    "write_jsonl(data, TRAINING_DATA_OUTPUT_FILE)\n",
    "print(\"✅ Generated training data:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5b1813-d5d0-4789-a8cb-507b52990db6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
